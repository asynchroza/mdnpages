.\" Automatically generated by Pandoc 3.1.11
.\"
.TH "OfflineAudioContext.startRendering" "JS" "November 21, 2023" "JavaScript" "JavaScript Reference Manual"
.SH NAME
OfflineAudioContext.startRendering \- OfflineAudioContext:
startRendering() method
.SH SYNOPSIS
The \f[CR]startRendering()\f[R] method of the
\f[CR]OfflineAudioContext\f[R] Interface starts rendering the audio
graph, taking into account the current connections and the current
scheduled changes.
.PP
The \f[CR]complete\f[R] event (of type
\f[CR]OfflineAudioCompletionEvent\f[R]) is raised when the rendering is
finished, containing the resulting \f[CR]AudioBuffer\f[R] in its
\f[CR]renderedBuffer\f[R] property.
.PP
Browsers currently support two versions of the
\f[CR]startRendering()\f[R] method \[em] an older event\-based version
and a newer promise\-based version.
The former will eventually be removed, but currently both mechanisms are
provided for legacy reasons.
.SH SYNTAX
.IP
.EX
startRendering()
.EE
.SS Parameters
None.
.SS Return value
A \f[CR]Promise\f[R] that fulfills with an \f[CR]AudioBuffer\f[R].
.SH EXAMPLES
.SS Playing audio with an offline audio context
In this example, we declare both an \f[CR]AudioContext\f[R] and an
\f[CR]OfflineAudioContext\f[R] object.
We use the \f[CR]AudioContext\f[R] to load an audio track
\f[CR]fetch()\f[R], then the \f[CR]OfflineAudioContext\f[R] to render
the audio into an \f[CR]AudioBufferSourceNode\f[R] and play the track
through.
After the offline audio graph is set up, we render it to an
\f[CR]AudioBuffer\f[R] using
\f[CR]OfflineAudioContext.startRendering()\f[R].
.PP
When the \f[CR]startRendering()\f[R] promise resolves, rendering has
completed and the output \f[CR]AudioBuffer\f[R] is returned out of the
promise.
.PP
At this point we create another audio context, create an
\f[CR]AudioBufferSourceNode\f[R] inside it, and set its buffer to be
equal to the promise \f[CR]AudioBuffer\f[R].
This is then played as part of a simple standard audio graph.
.RS
.PP
\f[B]Note:\f[R] You can \c
.UR
https://mdn.github.io/webaudio-examples/offline-audio-context-promise/
run the full example live
.UE \c
, or \c
.UR
https://github.com/mdn/webaudio-examples/blob/main/offline-audio-context-promise/
view the source
.UE \c
\&.
.RE
.IP
.EX
// Define both online and offline audio contexts
let audioCtx; // Must be initialized after a user interaction
const offlineCtx = new OfflineAudioContext(2, 44100 * 40, 44100);

// Define constants for dom nodes
const play = document.querySelector(\[dq]#play\[dq]);

function getData() {
  // Fetch an audio track, decode it and stick it in a buffer.
  // Then we put the buffer into the source and can play it.
  fetch(\[dq]viper.ogg\[dq])
    .then((response) => response.arrayBuffer())
    .then((downloadedBuffer) => audioCtx.decodeAudioData(downloadedBuffer))
    .then((decodedBuffer) => {
      console.log(\[dq]File downloaded successfully.\[dq]);
      const source = new AudioBufferSourceNode(offlineCtx, {
        buffer: decodedBuffer,
      });
      source.connect(offlineCtx.destination);
      return source.start();
    })
    .then(() => offlineCtx.startRendering())
    .then((renderedBuffer) => {
      console.log(\[dq]Rendering completed successfully.\[dq]);
      play.disabled = false;
      const song = new AudioBufferSourceNode(audioCtx, {
        buffer: renderedBuffer,
      });
      song.connect(audioCtx.destination);

      // Start the song
      song.start();
    })
    .catch((err) => {
      console.error(\[ga]Error encountered: ${err}\[ga]);
    });
}

// Activate the play button
play.onclick = () => {
  play.disabled = true;
  // We can initialize the context as the user clicked.
  audioCtx = new AudioContext();

  // Fetch the data and start the song
  getData();
};
.EE
.SH SEE ALSO
.IP \[bu] 2
Using the Web Audio API
