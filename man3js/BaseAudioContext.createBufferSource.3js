.\" Automatically generated by Pandoc 3.1.11
.\"
.TH "BaseAudioContext.createBufferSource" "JS" "November 28, 2023" "JavaScript" "JavaScript Reference Manual"
.SH NAME
BaseAudioContext.createBufferSource \- BaseAudioContext:
createBufferSource() method
.SH SYNOPSIS
The \f[CR]createBufferSource()\f[R] method of the
\f[CR]BaseAudioContext\f[R] Interface is used to create a new
\f[CR]AudioBufferSourceNode\f[R], which can be used to play audio data
contained within an \f[CR]AudioBuffer\f[R] object.
\f[CR]AudioBuffer\f[R]s are created using
\f[CR]BaseAudioContext.createBuffer\f[R] or returned by
\f[CR]BaseAudioContext.decodeAudioData\f[R] when it successfully decodes
an audio track.
.RS
.PP
\f[B]Note:\f[R] The \f[CR]AudioBufferSourceNode()\f[R] constructor is
the recommended way to create a \f[CR]AudioBufferSourceNode\f[R]; see
Creating an AudioNode.
.RE
.SH SYNTAX
.IP
.EX
createBufferSource()
.EE
.SS Parameters
None.
.SS Return value
An \f[CR]AudioBufferSourceNode\f[R].
.SH EXAMPLES
In this example, we create a two second buffer, fill it with white
noise, and then play it via an \f[CR]AudioBufferSourceNode\f[R].
The comments should clearly explain what is going on.
.RS
.PP
\f[B]Note:\f[R] You can also \c
.UR https://mdn.github.io/webaudio-examples/audio-buffer/
run the code live
.UE \c
, or \c
.UR
https://github.com/mdn/webaudio-examples/blob/main/audio-buffer/index.html
view the source
.UE \c
\&.
.RE
.IP
.EX
const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
const button = document.querySelector(\[dq]button\[dq]);
const pre = document.querySelector(\[dq]pre\[dq]);
const myScript = document.querySelector(\[dq]script\[dq]);

pre.innerHTML = myScript.innerHTML;

// Stereo
const channels = 2;
// Create an empty two second stereo buffer at the
// sample rate of the AudioContext
const frameCount = audioCtx.sampleRate * 2.0;

const myArrayBuffer = audioCtx.createBuffer(
  channels,
  frameCount,
  audioCtx.sampleRate,
);

button.onclick = () => {
  // Fill the buffer with white noise;
  //just random values between \-1.0 and 1.0
  for (let channel = 0; channel < channels; channel++) {
    // This gives us the actual ArrayBuffer that contains the data
    const nowBuffering = myArrayBuffer.getChannelData(channel);
    for (let i = 0; i < frameCount; i++) {
      // Math.random() is in [0; 1.0]
      // audio needs to be in [\-1.0; 1.0]
      nowBuffering[i] = Math.random() * 2 \- 1;
    }
  }

  // Get an AudioBufferSourceNode.
  // This is the AudioNode to use when we want to play an AudioBuffer
  const source = audioCtx.createBufferSource();
  // set the buffer in the AudioBufferSourceNode
  source.buffer = myArrayBuffer;
  // connect the AudioBufferSourceNode to the
  // destination so we can hear the sound
  source.connect(audioCtx.destination);
  // start the source playing
  source.start();
};
.EE
.SH SEE ALSO
.IP \[bu] 2
Using the Web Audio API
