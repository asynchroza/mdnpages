.\" Automatically generated by Pandoc 3.1.11
.\"
.TH "OfflineAudioContext" "JS" "November 21, 2023" "JavaScript" "JavaScript Reference Manual"
.SH NAME
OfflineAudioContext \- OfflineAudioContext
.SH SYNOPSIS
The \f[CR]OfflineAudioContext\f[R] interface is an
\f[CR]AudioContext\f[R] interface representing an audio\-processing
graph built from linked together \f[CR]AudioNode\f[R]s.
In contrast with a standard \f[CR]AudioContext\f[R], an
\f[CR]OfflineAudioContext\f[R] doesn\[cq]t render the audio to the
device hardware; instead, it generates it, as fast as it can, and
outputs the result to an \f[CR]AudioBuffer\f[R].
.SH CONSTRUCTOR
.TP
\f[B]OfflineAudioContext()\f[R]
Creates a new \f[CR]OfflineAudioContext\f[R] instance.
.SH INSTANCE PROPERTIES
\f[I]Also inherits properties from its parent interface,
\f[CI]BaseAudioContext\f[I].\f[R]
.TP
\f[B]OfflineAudioContext.length\f[R] \f[I](read\-only)\f[R]
An integer representing the size of the buffer in sample\-frames.
.SH INSTANCE METHODS
\f[I]Also inherits methods from its parent interface,
\f[CI]BaseAudioContext\f[I].\f[R]
.TP
\f[B]OfflineAudioContext.suspend()\f[R]
Schedules a suspension of the time progression in the audio context at
the specified time and returns a promise.
.TP
\f[B]OfflineAudioContext.startRendering()\f[R]
Starts rendering the audio, taking into account the current connections
and the current scheduled changes.
This page covers both the event\-based version and the promise\-based
version.
.SS Deprecated methods
.TP
\f[B]OfflineAudioContext.resume()\f[R]
Resumes the progression of time in an audio context that has previously
been suspended.
.RS
.PP
\f[B]Note:\f[R] The \f[CR]resume()\f[R] method is still available \[em]
it is now defined on the \f[CR]BaseAudioContext\f[R] interface (see
\f[CR]AudioContext.resume\f[R]) and thus can be accessed by both the
\f[CR]AudioContext\f[R] and \f[CR]OfflineAudioContext\f[R] interfaces.
.RE
.SH EVENTS
Listen to these events using \f[CR]addEventListener()\f[R] or by
assigning an event listener to the \f[CR]oneventname\f[R] property of
this interface:
.TP
\f[B]complete\f[R]
Fired when the rendering of an offline audio context is complete.
.SH EXAMPLES
.SS Playing audio with an offline audio context
In this example, we declare both an \f[CR]AudioContext\f[R] and an
\f[CR]OfflineAudioContext\f[R] object.
We use the \f[CR]AudioContext\f[R] to load an audio track
\f[CR]fetch()\f[R], then the \f[CR]OfflineAudioContext\f[R] to render
the audio into an \f[CR]AudioBufferSourceNode\f[R] and play the track
through.
After the offline audio graph is set up, we render it to an
\f[CR]AudioBuffer\f[R] using
\f[CR]OfflineAudioContext.startRendering()\f[R].
.PP
When the \f[CR]startRendering()\f[R] promise resolves, rendering has
completed and the output \f[CR]AudioBuffer\f[R] is returned out of the
promise.
.PP
At this point we create another audio context, create an
\f[CR]AudioBufferSourceNode\f[R] inside it, and set its buffer to be
equal to the promise \f[CR]AudioBuffer\f[R].
This is then played as part of a simple standard audio graph.
.RS
.PP
\f[B]Note:\f[R] You can \c
.UR
https://mdn.github.io/webaudio-examples/offline-audio-context-promise/
run the full example live
.UE \c
, or \c
.UR
https://github.com/mdn/webaudio-examples/blob/main/offline-audio-context-promise/
view the source
.UE \c
\&.
.RE
.IP
.EX
// Define both online and offline audio contexts
let audioCtx; // Must be initialized after a user interaction
const offlineCtx = new OfflineAudioContext(2, 44100 * 40, 44100);

// Define constants for dom nodes
const play = document.querySelector(\[dq]#play\[dq]);

function getData() {
  // Fetch an audio track, decode it and stick it in a buffer.
  // Then we put the buffer into the source and can play it.
  fetch(\[dq]viper.ogg\[dq])
    .then((response) => response.arrayBuffer())
    .then((downloadedBuffer) => audioCtx.decodeAudioData(downloadedBuffer))
    .then((decodedBuffer) => {
      console.log(\[dq]File downloaded successfully.\[dq]);
      const source = new AudioBufferSourceNode(offlineCtx, {
        buffer: decodedBuffer,
      });
      source.connect(offlineCtx.destination);
      return source.start();
    })
    .then(() => offlineCtx.startRendering())
    .then((renderedBuffer) => {
      console.log(\[dq]Rendering completed successfully.\[dq]);
      play.disabled = false;
      const song = new AudioBufferSourceNode(audioCtx, {
        buffer: renderedBuffer,
      });
      song.connect(audioCtx.destination);

      // Start the song
      song.start();
    })
    .catch((err) => {
      console.error(\[ga]Error encountered: ${err}\[ga]);
    });
}

// Activate the play button
play.onclick = () => {
  play.disabled = true;
  // We can initialize the context as the user clicked.
  audioCtx = new AudioContext();

  // Fetch the data and start the song
  getData();
};
.EE
.SH SEE ALSO
.IP \[bu] 2
Using the Web Audio API
