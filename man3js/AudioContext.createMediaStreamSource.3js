.\" Automatically generated by Pandoc 3.1.11
.\"
.TH "AudioContext.createMediaStreamSource" "JS" "November 29, 2023" "JavaScript" "JavaScript Reference Manual"
.SH NAME
AudioContext.createMediaStreamSource \- AudioContext:
createMediaStreamSource() method
.SH SYNOPSIS
The \f[CR]createMediaStreamSource()\f[R] method of the
\f[CR]AudioContext\f[R] Interface is used to create a new
\f[CR]MediaStreamAudioSourceNode\f[R] object, given a media stream (say,
from a \f[CR]MediaDevices.getUserMedia\f[R] instance), the audio from
which can then be played and manipulated.
.PP
For more details about media stream audio source nodes, check out the
\f[CR]MediaStreamAudioSourceNode\f[R] reference page.
.SH SYNTAX
.IP
.EX
createMediaStreamSource(stream)
.EE
.SS Parameters
.TP
\f[B]stream\f[R]
A \f[CR]MediaStream\f[R] to serve as an audio source to be fed into an
audio processing graph for use and manipulation.
.SS Return value
A new \f[CR]MediaStreamAudioSourceNode\f[R] object representing the
audio node whose media is obtained from the specified source stream.
.SH EXAMPLES
In this example, we grab a media (audio + video) stream from
\f[CR]navigator.getUserMedia\f[R], feed the media into a
\f[CR]<video>\f[R] element to play then mute the audio, but then also
feed the audio into a \f[CR]MediaStreamAudioSourceNode\f[R].
Next, we feed this source audio into a low pass
\f[CR]BiquadFilterNode\f[R] (which effectively serves as a bass
booster), then a \f[CR]AudioDestinationNode\f[R].
.PP
The range slider below the \f[CR]<video>\f[R] element controls the
amount of gain given to the lowpass filter \[em] increase the value of
the slider to make the audio sound more bass heavy!
.RS
.PP
\f[B]Note:\f[R] You can see this \c
.UR https://mdn.github.io/webaudio-examples/stream-source-buffer/
example running live
.UE \c
, or \c
.UR
https://github.com/mdn/webaudio-examples/tree/main/stream-source-buffer
view the source
.UE \c
\&.
.RE
.IP
.EX
const pre = document.querySelector(\[dq]pre\[dq]);
const video = document.querySelector(\[dq]video\[dq]);
const myScript = document.querySelector(\[dq]script\[dq]);
const range = document.querySelector(\[dq]input\[dq]);

// getUserMedia block \- grab stream
// put it into a MediaStreamAudioSourceNode
// also output the visuals into a video element

if (navigator.mediaDevices) {
  console.log(\[dq]getUserMedia supported.\[dq]);
  navigator.mediaDevices
    .getUserMedia({ audio: true, video: true })
    .then((stream) => {
      video.srcObject = stream;
      video.onloadedmetadata = (e) => {
        video.play();
        video.muted = true;
      };

      // Create a MediaStreamAudioSourceNode
      // Feed the HTMLMediaElement into it
      const audioCtx = new AudioContext();
      const source = audioCtx.createMediaStreamSource(stream);

      // Create a biquadfilter
      const biquadFilter = audioCtx.createBiquadFilter();
      biquadFilter.type = \[dq]lowshelf\[dq];
      biquadFilter.frequency.value = 1000;
      biquadFilter.gain.value = range.value;

      // connect the AudioBufferSourceNode to the gainNode
      // and the gainNode to the destination, so we can play the
      // music and adjust the volume using the mouse cursor
      source.connect(biquadFilter);
      biquadFilter.connect(audioCtx.destination);

      // Get new mouse pointer coordinates when mouse is moved
      // then set new gain value

      range.oninput = () => {
        biquadFilter.gain.value = range.value;
      };
    })
    .catch((err) => {
      console.log(\[ga]The following gUM error occurred: ${err}\[ga]);
    });
} else {
  console.log(\[dq]getUserMedia not supported on your browser!\[dq]);
}

// dump script to pre element

pre.innerHTML = myScript.innerHTML;
.EE
.RS
.PP
\f[B]Note:\f[R] As a consequence of calling
\f[CR]createMediaStreamSource()\f[R], audio playback from the media
stream will be re\-routed into the processing graph of the
\f[CR]AudioContext\f[R].
So playing/pausing the stream can still be done through the media
element API and the player controls.
.RE
.SH SEE ALSO
.IP \[bu] 2
Using the Web Audio API
